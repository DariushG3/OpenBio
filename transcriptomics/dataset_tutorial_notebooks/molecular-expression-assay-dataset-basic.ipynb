{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to working with Molecular Expression Datasets\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***As-Is Software Disclaimer***\n",
        "\n",
        "*This content in this repository is delivered “As-Is”. Notwithstanding anything to the contrary, DNAnexus will have no warranty, support, liability or other obligations with respect to Materials provided hereunder.*\n",
        "\n",
        "*[MIT License](https://github.com/dnanexus/OpenBio/blob/master/LICENSE.md) applies to this notebook.*\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Launch spec:**\n",
        "- App name: JupyterLab with Spark Cluster\n",
        "- Kernel: Python 3\n",
        "- Instance type: mem1_ssd1_v2_x16\n",
        "- Spark cluster configuration: single node\n",
        "- runtime: ~ 5 min\n",
        "\n",
        "**Package dependencies:** \n",
        "- pprint [License](https://docs.python.org/3/license.html?#psf-license)\n",
        "- pyspark [License](http://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "**Data description:** The record ID of a Dataset (or Cohort) that has an instance of a MolecularExpressionAssay. All molecular expression data in this notebook is synthetic.\n",
        "\n",
        "**This notebook shows how to:** Access molecular expression data from a Dataset.\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load packages using `import` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import dxdata\n",
        "import pprint\n",
        "import pyspark\n",
        "from pyspark.sql import functions as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "796d098c",
      "metadata": {},
      "source": [
        "#### Initiate Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f15ed328",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sc = pyspark.SparkContext()\n",
        "spark = pyspark.sql.SparkSession(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Dataset (or Cohort)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "dataset = dxdata.load_dataset(id=\"record-G7y66Vj0GjvK7xbp4Xg2Zjy5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### View attributes of the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'annotations': [],\n",
            " 'assays': DxOrderedDict([('my_expression_project',\n",
            "                           <dxdata.dataset.assay.MolecularExpressionAssay object at 0x7f666adbc5f8>)]),\n",
            " 'dashboards': OrderedDict(),\n",
            " 'edges': [<dxdata.dataset.dataset.Edge object at 0x7f666adbc278>,\n",
            "           <dxdata.dataset.dataset.Edge object at 0x7f666adbc358>],\n",
            " 'entities': [<Entity \"sample\">],\n",
            " 'entities_by_name': OrderedDict([('sample', <Entity \"sample\">)]),\n",
            " 'primary_dashboard': None,\n",
            " 'primary_entity': <Entity \"sample\">}\n"
          ]
        }
      ],
      "source": [
        "pprint.pprint(dataset.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### View Assay instances in the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DxOrderedDict([('my_expression_project',\n",
              "                <dxdata.dataset.assay.MolecularExpressionAssay at 0x7f666adbc5f8>)])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.assays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Select the desired assay and parse the Assay class for desired metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDefaultDict([('expr_annotation', <Entity \"expr_annotation\">),\n",
              "                    ('expression', <Entity \"expression\">)])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assay = dataset.assays[\"my_expression_project\"]\n",
        "assay.entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Identify the table and unique database name for the 'expression' Entity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "table = \"expression\"\n",
        "\n",
        "database_id = assay.entities[table].database_id.lower().replace('-', '_')\n",
        "database_name = assay.entities[table].database_name\n",
        "unique_database_name = f\"{database_id}__{database_name}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use Assay information to run query to extract molecular expression data into a Spark DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+---------+-----+\n",
            "|     feature_id|sample_id|value|\n",
            "+---------------+---------+-----+\n",
            "|ENST00000346162| sample_2|   85|\n",
            "|ENST00000438810| sample_1|   68|\n",
            "|ENST00000305531| sample_2|   88|\n",
            "|ENST00000427231| sample_3|   27|\n",
            "|ENST00000427970| sample_3|    4|\n",
            "|ENST00000290037| sample_3|   21|\n",
            "|ENST00000358472| sample_1|    5|\n",
            "|ENST00000398738| sample_1|    8|\n",
            "|ENST00000421624| sample_1|   90|\n",
            "|ENST00000397381| sample_2|   24|\n",
            "+---------------+---------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Build SQL\n",
        "sql = f\"SELECT * from {unique_database_name}.{table}\"\n",
        "\n",
        "# Build Spark DataFrame from SQL\n",
        "sdf = spark.sql(sql)\n",
        "sdf.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Convert long format table to count matrix\n",
        "*Note: For transformations resulting in tables with number of columns greater than the default setting of 10,000, it may be necessary to update the max value using spark.conf.set('spark.sql.pivotMaxValues', max_n). See Spark documentation at https://spark.apache.org/docs/latest/configuration.html#runtime-sql-configuration*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+--------+--------+--------+\n",
            "|     feature_id|sample_1|sample_2|sample_3|\n",
            "+---------------+--------+--------+--------+\n",
            "|ENST00000427970|      53|      34|       4|\n",
            "|ENST00000448786|      37|      74|      29|\n",
            "|ENST00000394713|      76|      76|      24|\n",
            "|ENST00000459639|      10|      78|      52|\n",
            "|ENST00000445980|      22|      46|      58|\n",
            "+---------------+--------+--------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Pivot to matrix format\n",
        "sdf_pivot = sdf.groupBy(\"feature_id\").pivot(\"sample_id\").agg(F.first(\"value\"))\n",
        "sdf_pivot.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### A Spark DataFrame may be written to a file, further analyzed using Spark  (or Koalas), or read into memory and analyzed using Pandas."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
